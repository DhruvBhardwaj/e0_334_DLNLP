{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JZdaS2pCWp2R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dbhar\\anaconda3\\envs\\e0_334_DLNLP\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OSMTjzFEWp2Z"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLIzZau9j89R",
        "outputId": "23f3ae44-55cc-46cf-bedb-89518cc34f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dV0BjTo2Wp2b"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4_b_Jl_eWp2c"
      },
      "outputs": [],
      "source": [
        "emb_model = {}\n",
        "f = open('glove.6B.300d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "  #print(line)\n",
        "  values = line.split()\n",
        "  try:\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    emb_model[word.lower()] = coefs    \n",
        "  except:\n",
        "    pass\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "emb_list = list(emb_model.keys())\n",
        "def create_embedding_matrix(embedding_dict,dimension):  \n",
        "  embedding_matrix=np.zeros((len(emb_list)+1,dimension))\n",
        "  \n",
        "  for idx,item in enumerate(emb_list):\n",
        "    embedding_matrix[idx]=embedding_dict[item]\n",
        "  return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "e = create_embedding_matrix(emb_model,300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(400001, 300)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "e.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_list.index('of')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.076947  , -0.021211  ,  0.21270999, -0.72232002, -0.13988   ,\n",
              "       -0.12234   , -0.17521   ,  0.12137   , -0.070866  , -1.57210004])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "e[3,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.076947, -0.021211,  0.21271 , -0.72232 , -0.13988 , -0.12234 ,\n",
              "       -0.17521 ,  0.12137 , -0.070866, -1.5721  ], dtype=float32)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_model['of'][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Truedddddd\n"
          ]
        }
      ],
      "source": [
        "a = -1*np.ones((len(emb_model['of']),1))\n",
        "b = -1*torch.ones((len(emb_model['of']),1))\n",
        "if ((torch.tensor(a)==b).sum() == 300):\n",
        "    print('Truedddddd')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IEcXvi67Wp2d"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=2\n",
        "SPLIT_RATIO = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "aZ_MOwCQWp2e"
      },
      "outputs": [],
      "source": [
        "def seq_to_sent_vec(seq):    \n",
        "    doc = nlp(re.sub('<[^<]+?>', '', seq))\n",
        "    eos_vec= -1*np.ones((len(emb_model['of']),1))\n",
        "    sentences=[]    \n",
        "    for token in doc.sents:        \n",
        "        ls = [t.text for t in token if (t.is_alpha==True and t.is_punct==False and t.like_url==False)]\n",
        "        print(' '.join(ls))\n",
        "        for word in ls:       \n",
        "            sentences.append(emb_model[word])\n",
        "        sentences.append(eos_vec)#<EOS>\n",
        "    return sentences\n",
        "\n",
        "def seq_to_vec(seq):    \n",
        "    doc = nlp(re.sub('<[^<]+?>', '', seq))\n",
        "    ls = [t.text for t in doc if (t.is_alpha==True and t.is_punct==False and t.like_url==False)]\n",
        "    #print(ls)\n",
        "    #print(len(ls))\n",
        "    vec=[]\n",
        "    for word in ls:\n",
        "        print(word)        \n",
        "        try:\n",
        "            vec.append(emb_list.index(word))\n",
        "        except:\n",
        "            pass    \n",
        "    return vec\n",
        "\n",
        "def label_str_to_num(label):    \n",
        "    if(label.lower()=='positive'):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One\n",
            "of\n",
            "the\n",
            "other\n",
            "reviewers\n",
            "has\n",
            "mentioned\n",
            "that\n",
            "after\n",
            "watching\n",
            "just\n",
            "Oz\n",
            "episode\n",
            "you\n",
            "be\n",
            "hooked\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[3, 0, 68, 17961, 31, 3042, 12, 49, 2641, 120, 1942, 81, 30, 12765]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_to_vec(\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "elayer = nn.Embedding.from_pretrained(torch.from_numpy(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One\n",
            "of\n",
            "the\n",
            "other\n",
            "reviewers\n",
            "has\n",
            "mentioned\n",
            "that\n",
            "after\n",
            "watching\n",
            "just\n",
            "Oz\n",
            "episode\n",
            "you\n",
            "be\n",
            "hooked\n",
            "torch.Size([1, 14])\n",
            "torch.Size([1, 14, 300])\n"
          ]
        }
      ],
      "source": [
        "v = seq_to_vec(\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked\")\n",
        "v = torch.IntTensor(v)\n",
        "v = v.unsqueeze(dim=0)\n",
        "print(v.size())\n",
        "x = elayer(torch.IntTensor(v))\n",
        "print(x.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([14, 300])\n"
          ]
        }
      ],
      "source": [
        "print(x.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "REqAI8X-Wp2i"
      },
      "outputs": [],
      "source": [
        "data_csv_str = \"Train dataset.csv\"\n",
        "data_df = pd.read_csv(data_csv_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
          ]
        }
      ],
      "source": [
        "print(data_df['review'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19808/4021730541.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq_to_sent_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19808/4267999023.py\u001b[0m in \u001b[0;36mseq_to_sent_vec\u001b[1;34m(seq)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mseq_to_sent_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<[^<]+?>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0meos_vec\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'of'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\dbhar\\anaconda3\\envs\\e0_334_DLNLP\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ],
      "source": [
        "s = seq_to_sent_vec(data_df['review'][0:2])\n",
        "print(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[16, 11, 24, 13, 12, 10, 17, 30, 15, 13, 22, 15, 43, 44, 22]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7uecFh7IWp2f"
      },
      "outputs": [],
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, text, labels):\n",
        "        self.labels = labels\n",
        "        self.text = text\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        text = self.text[idx]\n",
        "        sample = {\"Text\": text, \"Class\": label}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O2tZevW7Wp2g"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for sample in batch:        \n",
        "        label_list.append(label_str_to_num(sample['Class']))\n",
        "        processed_text = torch.tensor(seq_to_vec(sample['Text']), dtype=torch.float32)        \n",
        "        #label_list.append(sample['Class'])\n",
        "        #processed_text = torch.tensor(sample['Text'], dtype=torch.float32)\n",
        "        \n",
        "        text_list.append(processed_text)        \n",
        "        offsets.append(processed_text.size(0))\n",
        "        \n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "\n",
        "    #text_list = pad_sequence(text_list,batch_first=True)\n",
        "    #text_list=torch.tensor(text_list, dtype=torch.double)\n",
        "    #print(label_list.size(),offsets.size(),text_list.size())\n",
        "    \n",
        "    return label_list, text_list, offsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_h5ItL53Wp2j"
      },
      "outputs": [],
      "source": [
        "DS = CustomTextDataset(data_df['review'], data_df['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RnHIj0j5Wp2j"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(SPLIT_RATIO*len(DS))\n",
        "test_size = len(DS) - train_size\n",
        "\n",
        "train_dataset,val_dataset = random_split(DS, [train_size,test_size],generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True, collate_fn=collate_batch)\n",
        "val_dataloader = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([    1,    10,   100,  9999, 99999], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "some_list = [1, 10, 100, 9999, 99999]\n",
        "t = torch.from_numpy(np.array(some_list, dtype=np.int))\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "a={}\n",
        "a['word'] = 'abc'\n",
        "a['index'] = 222"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'abc'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17352/1444565435.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m: 'abc'"
          ]
        }
      ],
      "source": [
        "a['word']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VCQoZVxivgE"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.eos_marker = -1*torch.ones((len(emb_model['of']),1))\n",
        "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.hidden = self.init_hidden()\n",
        "    \n",
        "    def check_eos(self, x):\n",
        "        return ((x==self.eos_marker).sum() == self.input_dim)\n",
        "\n",
        "    def init_hidden(self):        \n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_dim)                            \n",
        "\n",
        "    def forward(self, x):\n",
        "        s = []\n",
        "        input = []\n",
        "        for i in range(0,x.size(0)):\n",
        "            if(~self.check_eos(x[i])):\n",
        "                _,(h,_) = self.lstm1(x)\n",
        "                print(h.size())\n",
        "                s.append(h)\n",
        "            else:\n",
        "                input = input.append(np.mean(np.array(s, dtype=np.float32),axis=0))\n",
        "        in1 = torch.tensor(np.array(input,dtype=np.float32))\n",
        "        _,(h,_) = self.lstm2(in1)\n",
        "        out = self.sigmoid(self.fc(h))\n",
        "        return out\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxQAZoA0mAjW"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def evaluate(model,criterion, val_loader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for _, (label, text, offset) in enumerate(val_loader):            \n",
        "            num_Samples = offset.size(0) \n",
        "\n",
        "            out=torch.zeros(offset.size(0),1)            \n",
        "            for idx in range(0,num_Samples):           \n",
        "              if(idx==num_Samples-1):\n",
        "                  x = text[offset[idx]:-1]\n",
        "              else:\n",
        "                  x = text[offset[idx]:offset[idx+1]]\n",
        "                             \n",
        "              if(x.size(0) != 0):          \n",
        "                  out[idx] = model(x.to(device))\n",
        "              else:\n",
        "                  out[idx] = 0.01\n",
        "            out = out.squeeze()                    \n",
        "            loss = criterion(out.to(device),label.to(device).float())       \n",
        "            \n",
        "            total_loss +=loss\n",
        "            total_acc += (out.round() == label).sum().item()\n",
        "            #print(total_acc)\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count, total_loss/total_count\n",
        "\n",
        "def train(train_loader, val_loader=None, learn_rate=0.01, hidden_dim=256, EPOCHS=5):\n",
        "    \n",
        "    # Setting common hyperparameters\n",
        "    input_dim = 300\n",
        "    output_dim = 1\n",
        "    n_layers = 1\n",
        "\n",
        "    model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    print(model)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Defining loss function and optimizer\n",
        "    criterion = nn.BCELoss(reduction='sum')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "    print('-' * 59)\n",
        "    print(\"Starting Training of model\")\n",
        "    epoch_times = []\n",
        "    # Start training loop\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        start_time = time.clock()        \n",
        "        total_loss = 0.\n",
        "        total_acc = 0.\n",
        "        counter = 0\n",
        "        model.train()\n",
        "        for _,(label, text, offset) in enumerate(train_loader):\n",
        "            counter += 1\n",
        "            model.zero_grad()              \n",
        "            num_Samples = offset.size(0) \n",
        "\n",
        "            out=torch.zeros(offset.size(0),1)            \n",
        "            for idx in range(0,num_Samples):           \n",
        "              if(idx==num_Samples-1):\n",
        "                  x = text[offset[idx]:-1]\n",
        "              else:\n",
        "                  x = text[offset[idx]:offset[idx+1]]\n",
        "                             \n",
        "              if(x.size(0) != 0):          \n",
        "                  out[idx] = model(x.to(device))\n",
        "              else:\n",
        "                  out[idx] = 0.01\n",
        "\n",
        "            out = out.squeeze()             \n",
        "            loss = criterion(out.to(device),label.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_acc += (out.round() == label).sum().item()\n",
        "            #print(out.size(),label.size())          \n",
        "            #print(out)\n",
        "            #print(label)\n",
        "            #print(total_acc)\n",
        "            \n",
        "            if counter%10 == 0:\n",
        "                print(\"Epoch {}......Step: {}/{}....... Average Loss={:10.4}:\".format(epoch, counter, len(train_loader), total_loss/(counter*BATCH_SIZE)))\n",
        "            \n",
        "\n",
        "        current_time = time.clock()\n",
        "        print(\"Epoch {}/{} Done, Average Train Loss={:10.4}, Train Accuracy={:10.4}\".format(epoch, EPOCHS, total_loss/(BATCH_SIZE*len(train_loader)),total_acc/(BATCH_SIZE*len(train_loader))))\n",
        "\n",
        "        if val_loader is not None:\n",
        "          acc_val,loss_val = evaluate(model, criterion, val_loader)\n",
        "          print(\"Epoch {}/{} Done, Average Val Loss={:10.4}, Average Val Accuracy={:10.4}\".format(epoch, EPOCHS, loss_val,acc_val))\n",
        "\n",
        "        print(\"Total Time Elapsed={} seconds\".format(str(current_time-start_time)))\n",
        "        epoch_times.append(current_time-start_time)\n",
        "        print('-' * 59)\n",
        "\n",
        "    print(\"Total Training Time={} seconds\".format(str(sum(epoch_times))))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCwRDTuzmARj",
        "outputId": "ad27ba9b-d574-432c-c471-17ba888dc627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTMNet(\n",
            "  (lstm): LSTM(300, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "-----------------------------------------------------------\n",
            "Starting Training of model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1......Step: 10/40....... Average Loss=     1.438:\n",
            "Epoch 1......Step: 20/40....... Average Loss=     1.078:\n",
            "Epoch 1......Step: 30/40....... Average Loss=    0.9526:\n",
            "Epoch 1......Step: 40/40....... Average Loss=    0.8942:\n",
            "Epoch 1/50 Done, Average Train Loss=    0.8942, Train Accuracy=    0.5363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 Done, Average Val Loss=     0.691, Average Val Accuracy=     0.525\n",
            "Total Time Elapsed=115.277061 seconds\n",
            "-----------------------------------------------------------\n",
            "Epoch 2......Step: 10/40....... Average Loss=    0.6965:\n",
            "Epoch 2......Step: 20/40....... Average Loss=    0.7006:\n",
            "Epoch 2......Step: 30/40....... Average Loss=    0.7201:\n",
            "Epoch 2......Step: 40/40....... Average Loss=    0.7274:\n",
            "Epoch 2/50 Done, Average Train Loss=    0.7274, Train Accuracy=    0.5188\n",
            "Epoch 2/50 Done, Average Val Loss=    0.7112, Average Val Accuracy=    0.5078\n",
            "Total Time Elapsed=113.82074200000011 seconds\n",
            "-----------------------------------------------------------\n",
            "Epoch 3......Step: 10/40....... Average Loss=    0.7007:\n",
            "Epoch 3......Step: 20/40....... Average Loss=    0.6922:\n",
            "Epoch 3......Step: 30/40....... Average Loss=    0.7016:\n",
            "Epoch 3......Step: 40/40....... Average Loss=     0.723:\n",
            "Epoch 3/50 Done, Average Train Loss=     0.723, Train Accuracy=    0.5215\n",
            "Epoch 3/50 Done, Average Val Loss=     0.716, Average Val Accuracy=    0.5375\n",
            "Total Time Elapsed=111.39101800000026 seconds\n",
            "-----------------------------------------------------------\n",
            "Epoch 4......Step: 10/40....... Average Loss=    0.7417:\n",
            "Epoch 4......Step: 20/40....... Average Loss=    0.7173:\n",
            "Epoch 4......Step: 30/40....... Average Loss=    0.7154:\n",
            "Epoch 4......Step: 40/40....... Average Loss=    0.7114:\n",
            "Epoch 4/50 Done, Average Train Loss=    0.7114, Train Accuracy=    0.5563\n",
            "Epoch 4/50 Done, Average Val Loss=    0.6878, Average Val Accuracy=    0.5344\n",
            "Total Time Elapsed=113.93687200000022 seconds\n",
            "-----------------------------------------------------------\n",
            "Epoch 5......Step: 10/40....... Average Loss=    0.6847:\n",
            "Epoch 5......Step: 20/40....... Average Loss=      0.71:\n",
            "Epoch 5......Step: 30/40....... Average Loss=    0.7131:\n",
            "Epoch 5......Step: 40/40....... Average Loss=    0.7084:\n",
            "Epoch 5/50 Done, Average Train Loss=    0.7084, Train Accuracy=    0.5602\n",
            "Epoch 5/50 Done, Average Val Loss=    0.7819, Average Val Accuracy=    0.5359\n",
            "Total Time Elapsed=114.07461400000011 seconds\n",
            "-----------------------------------------------------------\n",
            "Epoch 6......Step: 10/40....... Average Loss=    0.7203:\n",
            "Epoch 6......Step: 20/40....... Average Loss=    0.7282:\n",
            "Epoch 6......Step: 30/40....... Average Loss=    0.7134:\n",
            "Epoch 6......Step: 40/40....... Average Loss=    0.7039:\n",
            "Epoch 6/50 Done, Average Train Loss=    0.7039, Train Accuracy=     0.548\n",
            "Epoch 6/50 Done, Average Val Loss=    0.8283, Average Val Accuracy=    0.5281\n",
            "Total Time Elapsed=112.45974700000033 seconds\n",
            "-----------------------------------------------------------\n",
            "Epoch 7......Step: 10/40....... Average Loss=    0.6903:\n",
            "Epoch 7......Step: 20/40....... Average Loss=    0.6607:\n",
            "Epoch 7......Step: 30/40....... Average Loss=    0.6633:\n",
            "Epoch 7......Step: 40/40....... Average Loss=     0.671:\n",
            "Epoch 7/50 Done, Average Train Loss=     0.671, Train Accuracy=    0.6113\n",
            "Epoch 7/50 Done, Average Val Loss=    0.7626, Average Val Accuracy=    0.5625\n",
            "Total Time Elapsed=114.80663400000003 seconds\n",
            "-----------------------------------------------------------\n",
            "Epoch 8......Step: 10/40....... Average Loss=    0.6477:\n",
            "Epoch 8......Step: 20/40....... Average Loss=    0.6235:\n",
            "Epoch 8......Step: 30/40....... Average Loss=    0.6211:\n",
            "Epoch 8......Step: 40/40....... Average Loss=    0.6219:\n",
            "Epoch 8/50 Done, Average Train Loss=    0.6219, Train Accuracy=    0.6594\n",
            "Epoch 8/50 Done, Average Val Loss=    0.6707, Average Val Accuracy=    0.6203\n",
            "Total Time Elapsed=112.99616600000036 seconds\n",
            "-----------------------------------------------------------\n",
            "Epoch 9......Step: 10/40....... Average Loss=    0.5533:\n",
            "Epoch 9......Step: 20/40....... Average Loss=    0.6016:\n",
            "Epoch 9......Step: 30/40....... Average Loss=    0.6227:\n"
          ]
        }
      ],
      "source": [
        "Lstm_model = train(train_dataloader, val_dataloader, 0.1,256,50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKlFbxCFmADz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "E0_334_Assignment02 - Copy.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.12 ('e0_334_DLNLP')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "faf770d1e47eede554824f13152ff79334d378d32be76b65a9d49549787be319"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
